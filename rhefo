#!/bin/bash

#	Copyright (C) 2023-present SMAGGHE Cl√©ment (https://github.com/T-3B).
#
#	This program is free software: you can redistribute it and/or modify it under the terms of the Server Side Public License, version 1.
#
#	You should have received a copy of the Server Side Public License along with this program. If not, see <http://www.mongodb.com/licensing/server-side-public-license>.


# already tested dash, but exec time is mainly optimizers (shell commands interpretation time is neglible) and overcomplicates things (like I should write compressFile in another script file because cannot export function in dash - and env_parallel cannot use it)

#### Note on DeflOpt.exe and defluff (it was decided to not use them at all in this script):
#	Tested on a dozen .gz and .zip files (< 1MiB).
#	- defluff: can gain a few bytes (max 3) instantly after `ect -9 --mt-deflate` on rare cases (can be beaten with `ect -99999`), some times (more common) it adds a byte to the deflate-compressed stream, and most of the time no diff
#	- DeflOpt.exe: silently drops empty files from zip, never bigger output. In rare cases a few bytes (max 3) are saved (instantly again), but increasing ect's iterations beats it. Big issue: windows program.
#	for APNG, defluff can save about 0.07% of filesize instantly. I prefer to avoid these tools.

# for `dd`, if writing to existing output file, `dd status=none bs=4 count=1 seek=6 conv=notrunc oflag=seek_bytes iflag=fullblock of="$tmp"`

# features to add, fixes needed, etc are written with a 'TODO' in capital letters; 'todo' is written for stuff to change with very very low priority (mainly a nice thing to have, but almost useless because too rare), everything completely works without issues even with 'todo'
# formats (mime-types) which are completely finished and supported are marked 'DONE'


# TODO begin each section in case switch with a test (like some outputs of PDFs can be empty bc input is not supported or broken)
# also , still process if detected to have input problem, and say to user: warning, check this file (if not good, the backup is here ..)

# TODO some CLIs take all available threads (like ect), so write a function to reserve the threads for them with a semaphore with max value = glob_threads.
# useful for processing hundreds of PNGs (so less context switching and this is the best way to get outputs as fast as possible - if rhefo crash, we lost less work, as one file is processed at a time)

# TODO allow to stop and resume; using parallel --jl parallel.log --resume AND/OR criu (note that criu doesn't work on all kernels - and apparently needs setsid); take into account big ZIP optimization (many recursive calls of compressFile)

# TODO depending on whether we have lots of input files or not, run single-threaded jobs in parallel or a few jobs each taking multiple threads respectively (jpegultrascan -t x; ...). "input files < #threads" ?
# for that it is better to request/reserve the number of threads we want (and wait until we get all the threads we asked for) 

# TODO add globarchnoroot to remove root folder if there is only one folder
# TODO add option to remove __MACOSX folder from inside archives add to globarch-lossy and .DStore

# TODO for all the solid archive compressors, different file sortings will give different sizes (remember, do it too in TAR for .tar-based)
#		TODOOOOOOOOOOOOOO binsort (binsort use simhash + find the shortest path - something like the Voyageur de Commerce - so try implementing with tlsh and nilsimsa)
#		binsort wins by a large margin a greedy algorithm (take the closest file - with minimal distance - each time) with nilsimsa - didn't try tlsh as the CLI sucks

# TODO control memory: take only what's needed (backup, copy, ...) in TMPDIR, if taking more (decompressing archive, ...) do that in each case (specific to a mimie-type)
# put in a file what we can still use in mem to be safe, if not enough space wait until we can (can we do that with semaphores with parallel ?)
# => add a setting "globmaxmem" to limit the maximum memory used at the same time in the TMPDIR folder (parallel has some parameter to suspend jobs if not enough mem - search for "mem" in man page)

# TODO change zip-nodeflate to zip-broken (so use also flac-broken for flaccid); *-broken == better comp but (far) less compatibility/support
# TODO use jpg-broken for jpegtran -arithmetic

# TODO for the file tree struct in the glob-output, what about same as rsyn ?
# rsync source destination/   (copy folder `source` in destination - one more depth)
# rsync source/ destination/   (copy content of folder `source` - not itself)

# TODO allow mask to be mimetype if contains a '/' (like 'image/*')

# TODO what about how lossy and strip works ?
# do something like --<postfix>-strip where <postfix> is the last part of the filename (perhaps better if prefix of mimetype)
# do same hing about -insane
# TODO do the same thing for mask, like have a --<postfix>-ignore that won't do anything, and another one to simply copy from $in to $out

# TODO add warning if file changed between rhefo start and file opt

# TODO add progress to window title

# TODO break switch into different functions so easier to use local with meaningful names (local can be read by child fct call)

set -a  # export var+fct automatically
set -m  # All processes run in a separate process group (needed for Ctrl+C "trick")
version=0.0.1

command_not_found_handle() {
	warning "$1: command not found." 'Install the needed package and this script will resume automatically.'
	until command -v "$1" >/dev/null
	do sleep 3;
	done
	info "$1: found! Resuming optimization."
	"$@"
}

help=$(echo 'Really High Efficient File Optimizer (RHEFO), version '$version'. Under SSPL.
For updates, supported files, dependencies, issues, license and many more, see:
  https://github.com/T-3B/rhefo
Use this script with caution!
This script supports directories and files, but others (like symlinks) were NOT
tested (and I do NOT know if the script will work or "break" your machine)!

Usage: `rhefo [options] [--] INPUT [INPUT ...]`
INPUT can be folder or file, and several INPUTs can be given (arg and/or stdin).

List of options (global ones first, then grouped by file type):
--help | -h
  print help; use `-h` for options summary, `--help` for full help message
--extract-addins=<path> | -x <path>
  only extract add-ins embedded in this script to <path> then exit
--null | -0
  Use NUL as a delimiter instead of newline for inputs by stdin
--glob-downmix
  With audio, remove superfluous channels that are exact duplicates of another.
--glob-filemask=<mask> | -m <mask>
  only filenames matching the <mask> will be optimized (e.g. "*.mp3")
--glob-insane | -9	better compression
  if set, all "insane" flags are set too (no quality/compatibility loss)
  to "compress" better; consider --glob-lossy (only for lossy metadata);
  compatibility-break and lossy flags are NOT set; TAKES A LOT OF TIME!!!
--glob-jobs=<N> | -j<N>
  <N> files are optimized in parallel; default: same as --glob-threads
  more jobs mean more RAM usage, no impact on output filesize
--glob-keeptime | -k
  if set, input access and modification times are kept
--glob-lossy | -l	better compression, metalossy
  if set, all lossy flags are set (except the more agressive --glob-strip)
  lossy flags *only* act on metadata; most of time they remove what
    is not really needed (such as the version of FLAC encoder)
--glob-noextract | -E	worse compression, faster
  if set, do not compress embedded files (no "recursion")
--glob-nosort | -S
  if set, optimize files in the given order; default: sort by size (largest 1st)
--glob-output=<path> | -o <path>
  <path> is output folder; default: inplace (overwrite input files)
  even if set, --glob-tmp is still used
--glob-prefix=<prefix> | -p <prefix>
  Prefix to add to all processed files (--glob-output can still be used)
--glob-strip | -s	better compression, metalossy
  if set, strip/remove ALL metadata/exif; see the less aggressive --glob-lossy
--glob-threads=<N> | -t<N>
  <N> logic cpu processors used by this script; default: all (see --glob-jobs)
  more threads mean more RAM usage, no impact on output filesize
--glob-tmp=<path>
  <path> is a directory intensively used for r/w; default is `/dev/shm`
  if does not exist, or cannot r/w in it, defaults to `/tmp`
--glob-update | -y
  if set, will overwrite files in --glob-output
--flac-insane		better compression (~0.1% save), far slower (~200x)
  if set, run FLAC with the apodization function `subdivide_tukey(40)`
--flac-lossy		better compression, metalossy
  if set, vendor string from VORBIS_COMMENT is removed
    all blocks except VORBIS_COMMENT & PICTURE are removed (seektable,...)
    if there are no tags, then VORBIS_COMMENT block is removed too
  all PICTUREs are compressed with --glob-strip (drop all APICs metadata)
    PICTUREs type and description are kept, but not e.g. EXIF
--flac-subset		worse compression, faster
  if set, use subset mode (HW decode guaranteed); default: non-subset
--gzip-insane		better compression, far slower (TODO benchmark)
  if set, use 80199 iterations; default: 9
--gzip-lossy		better compression, metalossy
  if set, do not store the original decompressed filename
  default is to store it only if archive is not `originalName.gz`
--jpeg-insane		better compression, far slower (TODO benchmark)
  if set, bruteforce scans with both Moz/IJG jpegtran
--mp3-lossy		better compression (208 bytes less), metalossy
  if set, do not write a Xing frame; some softwares will show a wrong duration
--ogg-lossy		better compression, metalossy
  if set, vendor string from Vorbis streams are removed
--png-insane		better compression, far slower (TODO benchmark)
  if set, use 80199 iterations; default: 9 (PNG), 99 (APNG)
--zip-insane		better compression, far slower (TODO benchmark)
  if set, use 80199 iterations; default: 9
--zip-nodeflate		better compression, slower, compatibility issues
  if set, do not use only Deflate but also Deflate64,LZMA,PPMd,BZip2,Zstd,XZ
  default is to only use Deflate if input has only Deflate streams
  warning, many softwares do NOT support other stream but Deflate

If you think the script is taking too much time, do not hesitate to hit Ctrl+C!
The first Ctrl+C will tell RHEFO to not start any new job and to gently finish
currently running jobs. The second one will abruptly stop all jobs.
In both cases, temp files are cleaned and compression summary is printed.

Use `grep` with this help, it is easy! Try `rhefo | grep -e --flac -e --glob`.
Want further help/understanding on what/why really is done under the hood? Read
the comments in this script. You could learn how to compress even more than this
script will allow you (but you probably will compress 1 KB per minute).
Do not let me think I wrote these comments for nothing :¬¥)

Examples of usage:
# optimize recursively current dir, inplace
rhefo .
# optimize as much as possible (TAKES TIME!!!) recursively current dir, inplace,
# keep timestamps and remove some useless metadata
rhefo -9kl .
# optimize with only 2 threads, and copy /indir *inside* /outdir
rhefo -o /outdir -j2 /indir
# optimize 2 files with default settings and write output to an existing /outdir
rhefo -o /outdir <firstFile> <secondFile>
# optimize all FLAC files in /indir, recursively and in hidden dirs and
# output will be copied to /outdir (folders/files tree is recreated from /indir)
rhefo -o /outdir -m "*.flac" /indir
# same as above, but flattened (no subdirectories in /outdir)
find -name "*.flac" -type f -print0 | rhefo -0o /outdir' |\
	GREP_COLORS=mt=\;33 grep --color=always -E '^--[[:alnum:]_-]+|$' | GREP_COLORS=mt=\;36 grep --color=always -P '(?<= \| )-\w|$' | GREP_COLORS=mt=\;32 grep --color=always 'better compression\|$' | GREP_COLORS=mt=\;31 grep --color=always 'worse compression\|compatibility issues\|$' | GREP_COLORS=mt=1 grep --color=always -P '(?<=, )metalossy|$' | GREP_COLORS=mt=\;35 grep --color=always '^#.*\|$'
)

# Will parse arguments, and then start the whole optimization process (if no errors in the arguments)
parseArgs() {  # not using `local` vars because this fct is only executed once
	[ $(printf '\1' | od -dAn) = 1 ] || error 'This script was not tested on BIG ENDIAN devices. I mainly use `printf %x` with' 'numbers of up to 4 bytes.' 'PLEASE, open an issue and if you have time HELP ME supporting this architecture' 'by simply testing! This would really help a lot, since I only have little-endian' 'devices. Seriously, please.'
	getopt -T && { error 'Your `getopt` is not compatible with this script. Open an issue.' 4;}
	args=$(getopt -o hx:09j:klm:ESo:p:st:y -l help,extract-addins:,null,glob-downmix,glob-insane,glob-jobs:,glob-keeptime,glob-lossy,glob-filemask:,glob-noextract,glob-nosort,glob-output:,glob-prefix:,glob-strip,glob-threads:,glob-tmp:,glob-update,flac-insane,flac-lossy,flac-subset,gzip-insane,gzip-lossy,mp3-lossy,ogg-lossy,png-insane,zip-insane -n rhefo -s bash -- "$@")
	(($?)) && error 'Could not parse args. See the help message `rhefo -h`.'
	TMPDIR=/dev/shm  # mktemp uses directly TMPDIR, so set it instead of another variable globtmp (or we should specify this variable as an argument)
	eval set -- "$args"
	while :
	do case "$1" in
			-h)						echo "${help%%$'\n'*}"; echo "$help" | grep '\[;33m'; echo 'Run `rhefo --help` for the full help.'; exit;;
			--help)					echo "$help"; exit;;
			-x | --extract-addins)	[[ -d "$2" || -w "$2" ]] || error "Output directory '$2' does not exist or cannot write to it."; extractAddins "$2"; exit;;
			-0 | --null)			stdinnull=;;
			--glob-downmix)			globdownmix=;;
			-9 | --glob-insane)		flacinsane=;gzipinsane=;jpeginsane=;pnginsane=;zipinsane=;;
			-j | --glob-jobs)		((0<$2)) || error "Wrong jobs int: '$2'."; globjobs=$2; shift;;
			-k | --glob-keeptime)	globkeeptime=;;
			-l | --glob-lossy)		gziplossy=;flaclossy=;mp3lossy=;ogglossy=;;
			-m | --glob-filemask)	[ "$2" ] || error "Filemask is empty."; globfilemask=$2; shift;;
			-E | --glob-noextract)	globnoextract=;;
			-S | --glob-nosort)		globnosort=;;
			-o | --glob-output) 	[[ -d "$2" || -w "$2" ]] || error "Output directory '$2' does not exist or cannot write to it."; globoutput=$(realpath "$2" 2>/dev/null); shift;;
			-p | --glob-prefix)		[ "$2" ] || error 'Prefix is empty.'; globprefix=$2; shift;;
			-s | --glob-strip)		globstrip=;gziplossy=;flaclossy=;mp3lossy=;ogglossy=;;
			-t | --glob-threads)	((0<$2 && $2<=$(nproc --all))) || error "Wrong threads int: '$2'."; threadlimit $2; shift;;
			--glob-tmp)				[[ -d "$2" && -r "$2" && -w "$2" ]] && TMPDIR=$(realpath "$2") || { warning 'Could not use given --glob-tmp, using "/tmp" instead.'; TMPDIR=/tmp;}; shift;;
			-y | --glob-update)		globupdate=;;
			--flac-insane)			flacinsane=;;
			--flac-lossy)			flaclossy=;;
			--flac-subset)			flacsubset=;;
			--gzip-insane)			gzipinsane=;;
			--gzip-lossy)			gziplossy=;;
			--jpeg-insane)			jpeginsane=;;
			--mp3-lossy)			mp3lossy=;;
			--ogg-lossy)			ogglossy=;;
			--png-insane)			pnginsane=;;
			--zip-insane)			zipinsane=;;
			--zip-nodeflate)		zipnodeflate=;;
			--) 					shift; break;;
			*) 						error "Unexpected argument '$1'. See the help message \`rhefo -h\`."
		esac
		shift
	done
	[ -t 0 ] || mapfile -t${stdinnull+d ''}
	set -- "$@" "${MAPFILE[@]}"
	(($#)) || error 'No input given. See the help message `rhefo -h`.'
	for i do [ -r "$i" ] || error "Input '$i' does not exist or cannot read it."; done

	# create all the dir-tree to output folder
	[ -v globoutput ] && { for i do [ -d "$i" ] && { [ "${globoutput#$(realpath "$i")}" = "$globoutput" ] || error "The --glob-output folder can't be inside an input folder."; cd -- "$i"; find . -type d -exec mkdir -p "$globoutput/$(basename "$(realpath "$i")")/{}" \;; cd - >/dev/null;}; done;}
	TMPDIR=$(mktemp --tmpdir -d .rhefo_${version}_tmpXXX)

	printf '0\n0\n0\n' >"${globstatsfile=$(mktemp --tmpdir XXX)}"  # file where stats are wrote - to allow edit in subshells since we use `parallel`; 1st line is saved bytes, 2nd is input files size, 3rd is nbr input files

	addinsDir=$(mktemp -dp/tmp rhefo_${version}_tmpXXX)  # use /tmp for temp dir because we don't want weird chars (like ':') since we add it to PATH
	PATH=$addinsDir:$PATH
	extractAddins "$addinsDir"

	# get absolute path, find all files (print size, filepath, input folderpath), sort (biggest size first), compress in parallel
	{
		# TODO parallel --memsuspend
		# TODO parallel --noswap
		# TODO parallel --load to not start job if current cpu load is above x
		# TODO parallel --delay (makes sense to use only with --load)
		realpath -z -- "$@" | { cd /; parallel --xargs -q0 find {} -type f ${globfilemask+-name "$globfilemask"} -printf '%s\0%P\0%H\0' | { [ -v globnosort ] && cat || sortFilesBySize;} | parallel -j${globjobs-100%} -0L3 --lb --bar compressFile {2} {3} {1}; cd - >/dev/null;}
		# Thanks for the people helping me finding a way to sort by filesize (sorting by group of 3 elements): https://stackoverflow.com/a/76028193/13227011
		# TODO don't compressFile duplicates (explicitly twice same input or implicitely - with the file and the parent folder)

		((s=SECONDS, h=s/3600, m=s/60-h*60, s%=60))
		set -- $(<"$globstatsfile")
		info "$(printf "Congrats! Processed %d files in %s and saved %sB (%.3f%%)!\a" $3 $( ((h)) && printf ${h}h; ((m)) && printf ${m}m)${s}s $(numfmt --to=iec-i --format="%.2f" $1) $((1000000*$1/($2?$2:1)))e-4)"
		rm -r "$globstatsfile" "$addinsDir" "$TMPDIR"
	} &  # all the code is done in background so it won't be killed by Ctrl+C
	until wait; do :; done  # each Ctrl+C will kill `wait` (1st hit: no new job, 2nd: kill parallel)  # TODO remove --bar from parallel and print stuff ourself (including saved bytes/%)
}

# $1 = folder where to extract all the add-ins
extractAddins() {
	dd status=none bs=4k iflag=fullblock iflag=skip_bytes skip=$((6+$(grep -bm1 '^#EOF#$' "$(realpath -- "$BASH_SOURCE")" | cut -f1 -d:))) <"$(realpath -- "$BASH_SOURCE")" | base64 -d | tar xC "$1"
}

# restrict this script to use the $1 *least used* CPU processors
threadlimit() {
	# sum idle ($4) and io-wait ($5) ticks, substract between 2 reads (5 sec between), take $1 biggest numbers (will be our cpu threads to use with taskset)
	# could have used `taskset -acp 0-$(($1-1)) $$`, but doesn't "work" if > 1 instance of RHEFO is running
	[ $(nproc) = $(nproc --all) ] || { warning 'This script does NOT have access to all cpu threads, ignoring --glob-threads.'; return;}

	taskset -acp $(paste <(awk '/^cpu[0-9]/{print $4+$5}' /proc/stat) <(sleep 5; awk '/^cpu[0-9]/{print $4+$5}' /proc/stat) \
					| awk '{print $2-$1, NR-1}' | sort -n | tail -$1 | cut -d' ' -f2 | paste -sd,) $$ >/dev/null
}

# input should be `find -printf '%s\0%P\0%H\0'`, output will be sorted with largest %s first
sortFilesBySize() {
	perl -0ne 'push @rec, $_; if ($#rec == 2) { push @items, join("", @rec); @rec = (); } END { print(join("", sort { $b <=> $a } @items)) }'
}

# TODO better to change these2= 2 fct to only support reading data from stdin ?
# convert binary data to hexadecimal;	$1=filepath (or empty for stdin)	$2=index/offset of byte to print (in decimal)	$3=nbr of bytes to print (if no set then 1)
getBytes() {
	od --endian=big -An -j$2 -N${3-1} -vtx1 ${1:+"$1"} | tr -d ' \n'  # remove spaces and newline from output; for now everything I used only needed big endian
}

# $1=file;	$2=Bytes offset
synchsafeToInt() {
	echo $((0x$(getBytes "$1" $2)<<21 | 0x$(getBytes "$1" $((1+$2)))<<14 | 0x$(getBytes "$1" $((2+$2)))<<7 | 0x$(getBytes "$1" $((3+$2)))))
}

# convert int to binary data ("1" => 0x1) BIG endian;	$1=integer	$2=nbr of output bytes (padding with 0's if needed - "1" => 0x0000001)
intToBytes() {
	local i
	for i in $(seq $((8*$2-8)) -8 0)
	do printf $(printf '\\x%x' $(($1>>i&255)))
	done
}

# over all the input filepaths (having THE SAME NAME BEFORE LAST DOT '.'), find the smallest file, then rename it (with overwriting) without its last extension '.*', then delete all inputs
catchSmallestFile() {
	local a=$(find "$@" -printf '%s %f\0' | sort -nz | head -zn1 | tr -d \\0)  # need tr, otherwise we would have a warning msg for the trailing NULL byte
	a=${a#* }
	mv "$a" "${a%.*}"
	rm -f "$@"  # -f to not print err msg, since there's a missing file ("$a") 
}

# $1 = file to update
compressID3() {  # tested also tone and eyeD3; eyeD3 would be nice but python (and wait for this to release https://github.com/nicfit/eyeD3/issues/675)
	# eyeD3 --no-color --no-config --force-update --encoding utf8 --max-padding 0 --to-v2.4 --remove-v1 --remove-all-unknown --backup
	local picType descr id3Len i=$(kid3-cli -c 'get all 2' "$1" | grep -c '^  Picture')
	if [ -v mp3lossy ]
	then kid3-cli -c ls "$1" | grep -q '^> 1-' && kid3-cli -c 'syncto 2' -c save "$1"  # copy ID3v1 to ID3v2 if latter did not exist
		kid3-cli -c 'remove 1' -c save "$1"  # remove ID3v1
	fi
	kid3-cli -c 'config Tag.textEncoding 2' -c to24 -c textencoding -c save "$1" >/dev/null  # write to ID3v2.4 and 2=UTF-8
	while ((i--))
	do	picType=$(kid3-cli -c "get APIC[$i].pictureType 2" "$1")  # 2=apply to ID3v2
		descr=$(kid3-cli -c "get APIC[$i]:'$1.$i' 2" "$1")  # description + extract
		([ -v mp3lossy ] && export globstrip=; compressFile "$1.$i")
		kid3-cli -c "set APIC[$i]:'$1.$i' '$descr' 2" -c "set APIC[$i].pictureType $picType 2" -c save "$1"
	done
	if kid3-cli -c ls "$1" | grep -q '^> .2' && ! ((0x$(getBytes "$1" 5) & 2#10000))  # if ID3v2.4 is present and has no footer (if footer => no padding; idk if kid3 can write footer in output)
	then ((i=10,id3Len=$(synchsafeToInt "$1" 6)+10))  # headers in ID3v2.4 are synchsafe and 10B (not included in frame size)
		((0x$(getBytes "$1" 5) & 2#1000000 && (i+=$(synchsafeToInt "$1" 10))))  # optional extended header
		while ((i<id3Len))
		do	if ((0x$(getBytes "$1" $i)))
			then ((i+=$(synchsafeToInt "$1" $((i+4)))+10))
			else ((i-=10))
				{ dd status=none bs=6 count=1 iflag=fullblock; printf $(printf '\\x%x' $((i>>21)) $((i>>14 & 0x7F)) $((i>>7 & 0x7F)) $((i & 0x7F))); dd status=none bs=$i count=1 iflag=fullblock iflag=skip_bytes skip=4; dd status=none bs=4k iflag=fullblock iflag=skip_bytes skip=$((id3Len-i-10));} <"$1" >"$1.new"
				mv "$1.new" "$1"
				break
			fi
		done
	fi
}

# TODOOOOOOOOOOOOOOOO instead of no $3 if this is a subcall, use $SHLVL !!!!!!!!!!!!!!
# $1=filepath	$2=prefixpath	$3=filesize		("$2/$1" = full absolute input path)	(do NOT set $3 when this is a subcall from another compressFile - e.g. when compressFile a file from an extracted ZIP file); none can have spaces
compressFile() {	# warning: $1 can be empty
	[[ ! $3 && -v globnoextract ]] && return  # if this is a subcall compressFile and globnoextract is set, then do not optimize this file (as it's an embedded one)
	set -a  # export everything automaticall (e.g. sub-parallel calls)
	local a p v  # variables for temporary/intermediate values, ONLY SET THEM IN THE CASE SWITCH
	local atime mtime umask size in=$2${1:+${2:+/}}$1 tmp out
	atime=$(stat -c%x "$in")
	mtime=$(stat -c%y "$in")
	umask=$(stat -c%a "$in")
	cd "$(mktemp --tmpdir -d XXX)"  # keep every compressFile isolated from other threads, allows us to use "$tmp" with an additional filextension, or with some chars less, without worrying
	tmp=$(mktemp -p"$PWD" --suff=."${in##*/}" XXX)  # always use tmp, so in!=tmp
	#TODO check if $TMPDIR has enough free mem
	out=$in
	[ $3 ] && local inParent=$in
	[[ $3 && -v globoutput ]] && out=$globoutput/${2##*/}${1:+/$1}
	[[ $3 && -v globprefix ]] && out=${out%/*}/$globprefix${out##*/}
	if [[ $3 && (-v globoutput || -v globprefix) && ! -v globupdate && -e "$out" ]]
	then warning "'$out' already exists, file not compressed nor updated." 'Rerun with -y to overwrite files in output directory, or with -p to add a prefix.'
		cd /
		rm -r "${tmp%/*}"
		return
	fi
	size=${3-$(wc -c <"$in")}

	# TODO if mime=application/octet-stream, use FFmpeg to try to get the container name (or use a file mapping filextensions to mime-type)
	case $(file -b --mime-type "$in") in  # see here for known MIME types: /usr/share/mime/types
		application/gzip | application/x-gzip)	# DONE
			# tested tools: zopfli, zopfli MrKrzYch00's mod, gzip, 7z, minigzip (from minizip-ng). Tested on all files from silesia.tar (ect is far faster than zopfli(mod), and always gives smaller files - even when file is 1KB; ECT << zopfli < zopfli_mrkrzych00 << gzip); zopfli_mrkrzych00 --i9999 --mui999 --slowfix --t0 --pass9 --all androidsdk.svg
			p=$(gzip -Nl "$in")
			p=${p##*/}  # p= original decomp filename
			gzip -dc "$in" >"${tmp%.*}"
			compressFile "${tmp%.*}"
			gzip -1c$([[ -v gziplossy || "$p.gz" = "${tmp##*/}" ]] && echo n) "${tmp%.*}" >"$tmp"
			rm "${tmp%.*}"
			ect -${gzipinsane+8019}9 --mt-deflate -gzip --strict -quiet "$tmp";;  # using strict won't recompress embedded file
		application/pdf)  # TODO there is also `mutool extract`;	for lossless recompression check, see `mutool draw` (converts pdf to vector graphics)
			# TODO: best combination: cpdf -> pdfsizeopt -> mutool clean -> qpdf
			# TODO but another test: qpdf -> mutool -> pdfsizeopt
			# Warning it's not always the same order giving smallest file
			# cpdf: -remove-metadata -remove-duplicate-fonts
			mutool clean -gggg "$in" "$tmp"  # at least clean once
			p=$(wc -c <"$tmp")
			while v=$p; mutool clean -gggg "$tmp" "$tmp"; p=$(wc -c <"$tmp"); ((p<v)); do :; done  # repeat, until size stops decreasing, "garbage collect objects/streams + merge/reuse duplicates + compact cross ref table"; I saw a file that needed 7 iterations
			qpdf --recompress-flate --compression-level=9 --object-streams=generate --decode-level=specialized --replace-input "$tmp"  # don't use decode-level=all or bigger output
			# TODO for lossless check, see `mutool draw` (can convert pdf to vector graphics :chad:)
			# TODO check with pdfsizeopt (see minuimus for that, nice args), or see parser here : https://blog.didierstevens.com/programs/pdf-tools/ to extract objects
			# pdfsizeopt needed python2, ghostscript, sam2p, png22pnm
			# pdfsizeopt --optimize --use-pngout=yes --use-jbig2=yes --use-sam2p-pr=yes --use-image-optimizer='ect -999 -strip --mt-deflate %(targetfnq)s' in.pdf out.pdf
			;;
		application/octet-stream)  # can be many many things
			# TODO
			;;
		application/x-compress)
			flexiGIF -q --decompress "$in" "${tmp%.*}"
			compressFile "${tmp%.*}"
			flexiGIF -p -a=256 --compress "${tmp%.*}" "$tmp";;  # TODO insane: remove -a=256
		application/x-tar | application/x-cbt)  # TODO use binsort
			# some metadata infos are not guarenteed to be kept (owner, umask, atime/mtime)
			v=$(mktemp -d)
			tar xf "$in" -C"$v"
			compressTmpFolder "$v"
			find "$v" \( ! -type d -o -empty \) -printf %P\\0 | tar cf "$tmp" -b1 --no-recursion --null -C"$v" -T-  # use `find` to not store any prefix (`./`) in tar contents; by default `tar` uses 20x512 bytes block for alignment, use `-b1` for only one; do not store non-empty dir as a separate element in the tar
			rm -r "$v";;
		application/zip | application/zip-compressed | application/x-zip-compressed)
			# TODOOOOOOOOOOOO WARNING some uncompressed files need to stay uncompressed (like mimetype in epub - epub is a "disguised" ZIP). Use extension to differentiate EPUB from ZIP
			# TODO check application/epub+zip, application/vnd.comicbook+zip, application/x-zip-compressed-fb2,
			# TODO for 7z there are plug-ins : https://www.tc4shell.com/en/7zip/ (zstd, Brotli, LZ4, LZ5, Lizard, Fast LZMA2 and many more)
			# TODO  EPUBs, which require that the first entry in the ZIP be the mimetype file non-compressed
			#  TODO for android APK: "aur/apk-resigner 1-2 (+8 0.00)		A bash script utility that resigns the Android Package (APK) files (Android applications) with different certificates."
			# search "apk" or "android" with yay, because many utils exist to edit apk
			# tested softwares: minizip-ng, 7z, kzip, leanify, ect

			7z l -slt "$in" | grep '^Method = ' | grep -qve Store -e Deflate && p=  # at least one Method is not Store nor Deflate
			v=$(mktemp -d)
			7z x "$in" -o"$v" >/dev/null
			compressTmpFolder "$v"
			if [[ -v zipnodeflate || -v p ]]
			then : # TODO
			else  # even if `zip -9` only uses 100% cpu, compressed 2.2GB with 660 files in 67 seconds (fast enough). Could take less (but more mem) time by compressing each file in its own zip, ect all of them in parallel (because ect takes each file in the archive one after the other), then merge them (using zipmerge from libzip)
				cd "$v"
				base64 -d >"$tmp" <<<UEsFBgAAAAAAAAAAAAAAAAAAAAAAAA==  # empty zip
				zip -9rDq "$tmp" . |& grep -v 'zip error: Nothing to do!'  # -D to not add dirs; -9 forces everything > ~64 bytes to be compressed with Deflate (even if output is bigger) - useful as ect will NOT compress uncompressed files (and if Deflate stream is bigger than input data, ect will store the file uncompressed)
				find . -type d -empty -print0 | xargs -r0 zip -q "$tmp"
				cd - >/dev/null
				rm -r "$v"
				ect -${zipinsane+8019}9 -quiet -zip --strict --mt-deflate "$tmp"  # using --strict makes sure ECT does not try to optimize archived files (so it will only optimize Deflate compression in ZIP, not embedded files themselves)
			fi
			# TODO for apk, do not extract to optimize inside (would require apk-resigner - AUR; + zipalign; + some uncompressed files should stay uncompressed https://stackoverflow.com/q/25425172/13227011)

			# TODO for ZIP and 7z, we should bruteforce different compression method
			# ZIP: no (solid) block compression, so easier: each file is distinct from the others, so just bruteforce method compression and select the smallest embedded file. Deflate Deflate64 BZip2 LZMA XZ Zstd PPMd and perhaps more
				# minizip-ng: Deflate (no flag), Bzip2, LZMA, Zstd		7zip: Deflate, Deflate64 , BZip2
			;;
		audio/flac | audio/x-flac)  # Tested softwares (with different genres, 44.1 and 96 kHz, 16 and 24-bit, stereo): fork of reference FLAC 1.4.2 encoder https://github.com/ktmf01/flac; FLACOut.exe; CUETools.Flake.exe 2.2.2 --vbr 3 -11 -P 0 --lax --no-seektable -r 8 -e 32 -l 32 (tested different --window-method, -w, --vbr <0..4>, and did beat reference FLAC encoder *only* if I did *not* try -A functions); justinruggles' Flake
			# Note on FLAC compression: I made several benchmarks (10 encodes with different settings for 40 wav - PCM s16 44.1 kHz - files).
			# For easy copy-paste, here is a command. Nearly the same size as only using `subdivide_tukey(40)' (by ~0.01%), but taking 1.7x longer. Therefore if better compression ratio is needed I recommend only increasing the value given to `subdivide_tukey'.
			#flac in.flac -o out.flac -A subdivide_tukey\(25\) -A gauss\(0.1\) -A tukey\(25\) -A partial_tukey\(25\) -A punchout_tukey\(25\) -A welch -A bartlett -A bartlett_hann -A blackman -A blackman_harris_4term_92db -A connes -A flattop -A hamming -A hann -A kaiser_bessel -A nuttall -A rectangle -A triangle -8epP 0 --no-seektable -r 15 -l 32 --lax
			#subdivide_tukey(5);gauss(0.1);tukey(5);partial_tukey(5);punchout_tukey(5);welch;bartlett;bartlett_hann;blackman;blackman_harris_4term_92db;connes;flattop;hamming;hann;kaiser_bessel;nuttall;rectangle;triangle
			#flaccid --in liberate.flac --lax --out liberate_VarSizeBruteForced.flac --peakset-window 48 --preserve-flac-metadata --queue 8192 --workers 16 --tweak 1 --merge 0 --analysis-apod 'subdivide_tukey(5);gauss(0.1);tukey(5);partial_tukey(5);punchout_tukey(5);welch;bartlett;bartlett_hann;blackman;blackman_harris_4term_92db;connes;flattop;hamming;hann;kaiser_bessel;nuttall;rectangle;triangle' --output-apod 'subdivide_tukey(5);gauss(0.1);tukey(5);partial_tukey(5);punchout_tukey(5);welch;bartlett;bartlett_hann;blackman;blackman_harris_4term_92db;connes;flattop;hamming;hann;kaiser_bessel;nuttall;rectangle;triangle' --analysis-comp mepl32r15 --output-comp mepl32r15 --mode peakset --blocksize-list 512,1024,1536,2048,2560,3072,3584,4096,4608

			# TODO flac can be in ogg/oga!
			# TODO ID3 to VORBIS_COMMENT could be done with kid3-cli
			# TODO Ogg only supports Opus, Vorbis, FLAC, A-law PCM, Œº-law PCM, IEEE floating-point PCM, Speex and CELT.[e] OGMtools supports MP3 and AC-3 in Ogg.
			# TODO bruteforce blocksize (the size providing best ratio - mean - is 4096 with actual flags)

			# Warning! Do not use ffmpeg as it merges all tags with the same key within a single tag (with ';' as a separator).
			if ((2==$(metaflac --show-channels "$in") && 10==$(flac -sdc "$in" | sox - -n oops stat |& grep -c 0.000000)))  # if the input is fake stereo (left = right, strictly), then extract left channel
			then [ -v globdownmix ] && { flac -sdc "$in" | sox - -t wav - remix 1 | flac - -s0fP 0 --no-seektable -o "$tmp"
					metaflac --remove-all "$tmp"
					metaflac --list --data-format=binary --block-type=VORBIS_COMMENT,PICTURE,APPLICATION "$in" | metaflac --append "$tmp"
				} || cp "$in" "$tmp"
				info "'$in' is fake stereo: the 2 channels are bit-exact. ${globdownmix+Extracted one into mono output. The checksums will be different, but the operation is lossless!}${globdownmix-To extract it into a mono output, use --glob-downmix.}"
			else cp "$in" "$tmp"
			fi
			if ! [ -v globstrip ]
			then a=$(metaflac --list --block-type=PICTURE "$tmp" | grep -oP '(?<=^METADATA block #)\d+$')  # PICTURE blocks IDs
				# extract pictures (pics themselves and blocks), filenames written to array 'a'
				parallel 'metaflac --block-number={} --export-picture-to="$tmp".{} "$tmp"; metaflac --block-number={} --list --data-format=binary "$tmp" >"$tmp".{}.block' ::: $a
				metaflac --remove --block-type=PICTURE "$tmp"  # remove pics, since we append them later
				parallel --lb 's=$(wc -c <"$tmp".{}); [ -v flaclossy ] && export globstrip=; compressFile "$tmp".{}; t=$(wc -c <"$tmp".{}); { intToBytes $((0x$(getBytes "" 0 4)-s+t)) 4; head -c $(($(wc -c <"$tmp".{}.block)-s-8)); intToBytes $t 4; cat "$tmp".{};} <"$tmp".{}.block >"$tmp".{}.blockNew; rm "$tmp".{}{,.block}' ::: $a & # compress PICTURES in parallel with --glob-strip; Taking the first 4 bytes as block length even if the first one isn't, easier to implement and works. We update block length, image data length and image data.
				# now the {}.blockNew contains the opt picture, with correct size. todo Colors (the 4 bytes just before pic size) could be changed for example by ECT. Colors are the number of indexed colors; tried and everything works on PC if it's not updated; we could use `identify -verbose "$tmp"` and look at "Colormap entries:", if not in output then not indexed (at least for PNG), would add a dependency I don't want. Could also remove all block from FLAC, try to import pic and let FLAC figure it out (Colors) for me then copy (too much work for what it brings)
			fi
			# TODO -j only if 1.5.0
			flac "$tmp" -j $(nproc) -o "$tmp".new -smepb 4096 -P 0 -r ${flacsubset+8 -l $(($(metaflac --show-sample-rate "$tmp")>48000?32:12))} ${flacsubset-15 -l 32 --lax} -A subdivide_tukey\(4${flacinsane+0}\)
			flac -sa "$tmp" "$tmp".new
			v=$(($(tac "${tmp%.*}".ana | grep -oPm1 '(?<=\soffset=)\d+') - $(grep -oPm1 '(?<=\soffset=)\d+' "${tmp%.*}".ana) + $(tac "${tmp%.*}".ana | grep -oPm1 '(?<=\sbits=)\d+') / 8))  # to get the FLAC stream size, we substract 1st and last frames' offset, then add the length of the last frame (we could have used `metaflac --remove-all` but too much mem-hungry)
			p=$(($(tac "$tmp".ana      | grep -oPm1 '(?<=\soffset=)\d+') - $(grep -oPm1 '(?<=\soffset=)\d+' "$tmp".ana     ) + $(tac "$tmp".ana      | grep -oPm1 '(?<=\sbits=)\d+') / 8))
			((v>p)) && mv "$tmp".new "$tmp" || rm "$tmp".new
			if [ -v globstrip ]
			then metaflac --remove-all "$tmp"  # here and not before flac command because will add VORBIS_COMMENT even if empty
			elif [ -v flaclossy ]
			then metaflac --remove --except-block-type=VORBIS_COMMENT,PICTURE "$tmp"
				if metaflac --list --block-type=VORBIS_COMMENT "$tmp" | grep -q '^\s\scomments: 0$'
				then metaflac --remove --block-type=VORBIS_COMMENT "$tmp"
				else v=$(metaflac --show-vendor-tag "$tmp")
					p=$(($(grep -abom1 "$v" "$tmp" | head -1 | cut -f1 -d:)-7))  # the first match is the vendor-string, use head because -om1 can output more than 1 match
					{ dd status=none bs=$p count=1 iflag=fullblock; ((p=0x$(getBytes '' 0 3)-${#v})); intToBytes $p 3; intToBytes 0 4; dd status=none bs=4k iflag=fullblock iflag=skip_bytes skip=$((4+${#v}));} <"$tmp" >"$tmp".new  # TL;DR: 0xaaabbbb<vendorString>; aaa length metadata block in big endian, bbbb length vendor string in little
					mv "$tmp".new "$tmp"
				fi
			fi
			[ -v a ] && { wait; for p in $a; do metaflac --append "$tmp" <"$tmp".$p.blockNew; rm "$tmp".$p.blockNew; done;}  # we add PICTUREs back to the FLAC file
			metaflac --remove --block-type=PADDING --dont-use-padding "$tmp";;
		audio/mpeg | audio/mp3 | audio/mpeg3 | audio/x-mpeg-3)
			# MP3Packer will write "mp3packer2.04-268\n" in the NEEDED padding of frames
			# do not use FFmpeg because it converts ID3 tags to TXXX (comment: https://trac.ffmpeg.org/ticket/8996; lyrics: https://trac.ffmpeg.org/ticket/8795; perhaps others); FFmpeg 6.0 will ALWAYS write at least 10 bytes of padding (even if no ID3 metadata); AND IT CAN BE LOSSY EVEN WHEN COPYING AUDIO https://trac.ffmpeg.org/ticket/10995
			# TODO remove Xing/LAME frame if mp3lossy
			mp3packer -z --workers 1 -f "$in" "$tmp" >/dev/null  # mp3packer always adds a Xing/LAME frame
			if [ -v globstrip ]
			then mp3packer -s -t -f -u "$tmp" "$tmp.old" >/dev/null  # remove everything that's not the MP3 stream itself  # TODO still does not remove side metadata (see file)
			else compressID3 "$tmp"
			fi;;
		audio/ogg | video/ogg)
			# TODO rn only works on Vorbis streams, but would be cool to at least remove metadata for others
			# do not use oggz-comment to remove metadata as it'll inflate file by chaing packets/page
			if [[ $(oggz-codecs "$in") = vorbis && ${a=$(oggz-codecs -a "$in")} != vorbis ]] 2>/dev/null  # oggz-codecs prints a newline char on stderr...
			then warning "'$in': can not mux streams properly since there are several Vorbis streams and nothing else."
				cp "$in" "$tmp"  # todo: oggz-merge does not support a simple mux - without interleaving pages - for multiple vorbis streams (and nothing else), would print "oggz-merge: WARNING: Merging Ogg Vorbis I files [...]"
			else oggz-info "$in" | awk '/serialno/ {i++} /Vorbis/ {print i-1}' | parallel 'oggz-rip -i {} -o "$tmp".{}.ogg "$in"; optivorbis -q ${ogglossy+--vendor_string_action empty} ${globstrip+--comment_fields_action delete} "$tmp".{}.ogg "$tmp".{}_opt.ogg; rm "$tmp".{}.ogg'  # extract and optimize Vorbis streams
				[ $a = vorbis ] && mv "$tmp".* "$tmp" || {
					oggz-rip $(printf -- '-c %s ' $(oggz-known-codecs | grep -v Vorbis)) -o "$tmp".tmp.ogg "$in"  # extract all others streams
					oggz-merge -o "$tmp" "$tmp".*
				} 
			fi
			rm -f "$tmp".*;;
		audio/x-wavpack)  # FFmpeg will be slightly smaller than wavpack official encoder (for one thing, it lacks the WavPack 5 block checksums - but maybe not only? idk)
			;;
		image/gif)
			# gifski and ImageMagick (-quality 100 -layers optimize) cannot recompress losslessly
			# gifsicle and flexiGIF both can produce bigger output than input, only gifsicle *then* flexiGIF seems useful
			# TODO gifsicle
			# gifsicle -o 1"$i" -O3 --no-comments --no-extensions --no-app-extensions --no-names input.gif
			;;
		image/jpeg)
			# tested jpegtran ect leanify pingo jpegoptim curtail jpgcrush jhead jpegultrascan.pl; JPG optimizer benchmark: https://encode.su/threads/3987-Jpeg-Optimizer-testing

			# TODO for Exif editing: if $globstrip, remove everything except these to be completely lossless:
			# - ColorSpace, InteropIndex, ICC_Profile, Orientation; maybe others ? See https://exiftool.org/TagNames/EXIF.html
			# Try to `jpegtran -rotate x -perfect -outfile out.jpg in.jpg` with rotation written in Exif (WARNING may also be mirrored), will exit with $?=1 if not lossless transformation  (CHECK ALSO `ect -autorotate`)
			# TODO add -arithmetic coding with $jpgbroken

			# TODOOOOOOOOOOOOOOOOO copy icc will drop the rotation of the image!!!!!!!!!!!!!!!!!!!!!!

			# TODO `jhead -du` Or -purejpg which is a combination of params including -du; this command allowed Fox Wizard to beat me again (6 bytes smaller - metadata - than `ultrascan -b 13 -t 16 -a -i -s` !arithmetic!, ECT nor jpegtran could remove these 6 bytes)
			if [ -v jpeginsane ]
			then jpegultrascan.pl -qib3 ${globstrip+-s} -t$(nproc) -pjpegtran_ijg -ojpegtran_moz "$in" "$tmp"  # -b3 is really rarely beaten by high numbers
			else jpegtran_moz -copy all -outfile "$tmp" "$in" && ect -9 -progressive -quiet ${globstrip+-strip} "$tmp"
			fi;;
		image/jxl)
			# -e 10 needs libjxl 0.8.0
			# cjxl -e 9 -d 0 --brotli_effort=11 -I 100 -g 3 -E 11
			# bruteforcing some settings will compress better (-g 1..3, --patches, ...): https://github.com/libjxl/libjxl/issues/426#issuecomment-898962219
			# -e 10 will actually really bruteforce everything/every flag, no need for more than `-e 10 --allow_expert_options` (if cjxl uses more than 1 thread, no impact on the size) (more args is completely useless)

			# strip = '--container=0 --jpeg_store_metadata=0' and perhaps more
			# `-x strip=exif` was added fairly recently, might work with `-x strip=all` but not sure, could try exif and xmp at the same time too
			# code stripping container : https://github.com/Fraetor/jxl_decode/blob/main/src/jxl-strip.py

			# jxlinfo prints 'JPEG bitstream reconstruction data available', perhaps with glob-lossy remove that reconstruction (if does not degrade JXL quality - I guess it only change jpg reconstruction)

			#upscaled pixel art is very hard to beat.
			#You resize the art back down to 1:1 pixels, then with lossless you add
			#--already_downsampled --resampling=(2, 4 or 8, whichever is closest to the original) --upsampling_mode=0

			# JXL max effort is now 11 with allow_expert_options, 10 otherwise

			# For JPG lossless recompression, JXL does not *ALWAYS* output smaller file if input JPG is smaller (to "inflate" the JPG file, use `jpegtran -revert`
			;;
		image/png | image/x-png | image/vnd.mozilla.apng)
			# Tested software: ECT, leanify, zopflipng_mrkrzych00; zopflipng_mrkrzych00 -y -m --iterations=999 --slowfix --all --lossy_transparent --alpha_cleaners=bhvapw --pass=999 in out
			# ECT is the best Deflate encoder (looking at compression ratio only), so the only way to beat it is to bruteforce even more filters than `ect --allfilters-b --pal_sort=120` (not using this as it takes ages), but then if we find a better filter we could run ect on it and it will reuse the filter if gives better ratio
			# TODO can apngopt sometimes do lossy things (iirc it will change the pixels to rgba(pc)) ?
			# TODO remove useless alpha channel
			# TODO pngout apparently can sometimes do better: Just running pngout followed by pngout /f0 will result in the optimal compression; set the size manually can help Low entropy = High values, High entropy = Small values
			# TODO see (or better version than) optipng -fix -f 5 -i 0 -zc 9 -zm 9 -zs 0-3 -zw 32k -nb -nc -np -nx "$file"
			if [ $(ffprobe -v warning -show_entries stream=codec_name -of default=nk=1:nw=1 "$in") = apng ]
			then # ect -1 ${globstrip+-strip} "$tmp"  # TODO (cannot strip using ECT since it drops all other frames - only 1st one is kept)
				apngopt -z2 -i99${pnginsane+999} "$in" "$tmp" >/dev/null  # TODO to do better, we could split frames, use ECT --reuse, them remux   ;; TODO try pingo
			else cp "$in" "$tmp.png"  # TODO might get better compression with pngOUT (first run pngout without options and then pngout /f0)
				ect -${pnginsane+8019}9 --mt-deflate -quiet ${globstrip+-strip} "$tmp.png"  # ect needs correct extension
				# If the value is above 10000, the blocksplitting-compression cycle is repeated # / 10000 times. If # % 10000 is above 9, level 9 is used and the number of iterations of deflate compression per block is set to # % 10000. If # % 10000 is 9 or below, this number specifies the level.
				mv "$tmp.png" "$tmp"
			fi;;
		image/webp)  # only tested cwebp; cwebp -z9 is NOT always the smallest one! Therefore just bruteforce -z (which is an alias for -m and -q options)
			if webpinfo "$in" | grep -q 'Format: Lossless (2)'
			then parallel cwebp -quiet -lossless -alpha_filter best -metadata ${globstrip+icc}${globstrip-all} -z {} "$in" -o "$tmp".{} ::: {0..9}
				catchSmallestFile "$tmp".*
			fi;;
		*)
			# application/x-executable (TODO test if this MIME type is the only one for executables)
			#upx -9 --ultra-brute -o out in
			cp "$in" "$tmp";;  # TODO also cp in -> tmp IF THERE IS ANY ERROR (what about set -e or something like that?)
	esac
	((size_tmp=$(wc -c <"$tmp"),(size-=size_tmp)<=0)) && { info "'$in'$([ $3 ] || echo " (embedded file from '$inParent')"): could not compress further. Input file was kept."; cp "$in" "$tmp"; size=0;}
	((size_tmp)) || { warning "'$in'$([ $3 ] || echo " (embedded file from '$inParent')"): empty output! Report the issue. Input file was kept."; cp "$in" "$tmp"; size=0;}
	chmod $umask "$tmp"
	[ -v globkeeptime ] && touch -md"$mtime" "$tmp" && touch -ad"$atime" "$tmp"
	if [ $3 ]
	then sem --fg --id "$globstatsfile" 'set -- $(<"$globstatsfile"); printf "$(('$size'+$1))\n$(('$3'+$2))\n$((1+$3))\n" >"$globstatsfile"'  # use a semaphore to allow only one thread to write stats at a time
		if [[ -v globoutput && ! -v globupdate && -e "$out" ]]
		then warning "'$out' already exists, not updated. Compressed version here: '$tmp'." 'Rerun with -y to overwrite files in output directory, or with -p to add a prefix.'  # TODO not remove with below rm -r
		else mv "$tmp" "$out"
			chown -f --reference="$in" "$out"
			chmod -f --reference="$in" "$out"
		fi
	else mv "$tmp" "$out"
	fi
	cd /
	rm -r "${tmp%/*}"
}

# only one argument, the folder where we extracted the archive contents
compressTmpFolder() {
	[ -v globnoextract ] || find "$1" -type f -printf '%s\0%P\0%H\0' | sortFilesBySize | parallel -0L3 --lb globkeeptime= compressFile {2} {3}
}

info() { printf "\r\033[1;36mI: %s\033[0m\n" "$@";}

warning() { printf "\r\033[1;33mW: %s\033[0m\n" "$@" >&2;}

error() {  # TODO change this to have exit code as first argument (so we could simply use shift)
	if [[ ${@: -1} = +([0-9]) ]]  # check if last arg is a number (== exit code)
	then printf "\r\033[1;31mE: %s\033[0m\n" "${@:1:$#-1}" >&2; exit ${@: -1}
	else printf "\r\033[1;31mE: %s\033[0m\n" "$@" >&2; exit 1
	fi
}

ctrlCHandler() {
	if [ -v ctrlc ]
	then [ $ctrlc ] || kill $(pstree -ap $$ | grep -m1 parallel | grep -oP '^.*?\K[0-9]+') 2>/dev/null  # we could use `exit 130`, but it's better to only kill `parallel` so we can print information about already processed files (which is done really fast)
		ctrlc=g  # only allow 2 Ctrl+C signals, after that we simply catch and ignore them
	else
		ctrlc=
		{	kill -HUP $(pstree -ap $$ | grep parallel | grep -oP '^.*?\K[0-9]+') 2>/dev/null  # do NOT start any new job in any `parallel` (even grandchild processes)
			sleep .01  # so the following msg will be shown *after* parallel's output about no new job
			printf "\r%${COLUMNS}s\r" ''  # erase progress bar from parallel
			info 'Finishing running jobs, hit Ctrl+C again to stop them.'
		} &  # run in background so if we hit **again** Ctrl+C super fast (< .01 sec), ctrlCHandler won't be killed
	fi
}

trap ctrlCHandler INT  # Set up the Ctrl+C signal handler

parseArgs "$@"


exit
# add-ins encoded in base64 below this eof line (could append them in binary data, smaller file but many text editors don't support NULL bytes)
#EOF#
